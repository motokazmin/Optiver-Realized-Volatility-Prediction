{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom transformers import RobertaConfig, AdamW\nfrom transformers import RobertaTokenizerFast\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nimport pickle\nimport torch\nfrom torch import cuda\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-26T21:24:08.532889Z","iopub.execute_input":"2021-09-26T21:24:08.53325Z","iopub.status.idle":"2021-09-26T21:24:14.73199Z","shell.execute_reply.started":"2021-09-26T21:24:08.533171Z","shell.execute_reply":"2021-09-26T21:24:14.731166Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"delta = 0.0002\nwap1_min = 0.8830628395080566\nwap1_max = 1.1270768642425537","metadata":{"execution":{"iopub.status.busy":"2021-09-26T21:24:22.706487Z","iopub.execute_input":"2021-09-26T21:24:22.706811Z","iopub.status.idle":"2021-09-26T21:24:22.712975Z","shell.execute_reply.started":"2021-09-26T21:24:22.706774Z","shell.execute_reply":"2021-09-26T21:24:22.712161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_test():\n    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    return test\n\nclass Triage(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len, isSubmit = False):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.isSubmit = isSubmit\n        \n    def __getitem__(self, index):\n        history = str(self.data.history[index])\n        inputs = self.tokenizer.encode_plus(\n            history,\n            None,\n            max_length=self.max_len,\n            padding = 'max_length',            \n            return_attention_mask=True,\n            truncation=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        if self.isSubmit:\n            row_id = self.data.row_id[index]\n            return {\n                'ids': torch.tensor(ids, dtype=torch.long),\n                'mask': torch.tensor(mask, dtype=torch.long),\n                'row_id'  : row_id\n            } \n        else:\n            return {\n                'ids': torch.tensor(ids, dtype=torch.long),\n                'mask': torch.tensor(mask, dtype=torch.long),\n                'targets': torch.tensor(self.data.target[index], dtype=torch.float)\n            } \n    \n    def __len__(self):\n        return self.len\n    \nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = RobertaConfig.from_pretrained(model_name)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = RobertaForMaskedLM.from_pretrained(model_name, config=config)  \n            \n        self.attention = torch.nn.Sequential(            \n            torch.nn.Linear(768, 512),            \n            torch.nn.Tanh(),                       \n            torch.nn.Linear(512, 1),\n            torch.nn.Softmax(dim=1)\n        )        \n\n        self.regressor = torch.nn.Sequential(                        \n            torch.nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)\n    \ndef create_optimizer(model):\n    named_parameters = list(model.named_parameters())    \n    \n    roberta_parameters = named_parameters[:197]    \n    attention_parameters = named_parameters[199:203]\n    regressor_parameters = named_parameters[203:]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    regressor_group = [params for (name, params) in regressor_parameters]\n\n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": regressor_group})\n\n    for layer_num, (name, params) in enumerate(roberta_parameters):\n        weight_decay = 0.0 if \"bias\" in name else 0.01\n\n        lr = 2e-5\n\n        if layer_num >= 69:        \n            lr = 5e-5\n\n        if layer_num >= 133:\n            lr = 1e-4\n\n        parameters.append({\"params\": params,\n                           \"weight_decay\": weight_decay,\n                           \"lr\": lr})\n\n    return torch.optim.AdamW(parameters)\n\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef log_return(series):\n    return np.log(series).diff()\n\ndef book_preprocessor(delta, stock_id):\n    file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n\n    df = pd.read_parquet(file_path_book)[['time_id', 'seconds_in_bucket', 'bid_price1', 'ask_price1', 'bid_size1', 'ask_size1']]\n    df['wap1'] = calc_wap1(df)\n    \n    df.loc[df.wap1 < wap1_min, 'wap1'] = wap1_min\n    df.loc[df.wap1 > wap1_max, 'wap1'] = wap1_max\n\n    df['row_id'] = str(stock_id) + '-' + df['time_id'].astype(str)\n    df['i'] = ((df['wap1'] - wap1_min)/delta).astype(int)\n    return df.groupby(['row_id']).apply(convert_to_str)\n    \ndef convert_to_str(g):\n    start = 0\n    if len(g) > 590:\n        start = 10\n    s = ''.join(str(g.i.values[start:])).replace('\\n', '')\n    return s[1: len(s) - 1] + ' . '","metadata":{"execution":{"iopub.status.busy":"2021-09-26T21:24:24.942154Z","iopub.execute_input":"2021-09-26T21:24:24.942516Z","iopub.status.idle":"2021-09-26T21:24:24.967709Z","shell.execute_reply.started":"2021-09-26T21:24:24.942484Z","shell.execute_reply":"2021-09-26T21:24:24.966643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = '../input/optiver-realized-volatility-prediction/'\n\ntest = read_test()\ntest_stock_ids = test['stock_id'].unique()\n\ntest_dataset = pd.DataFrame()\nfor stock_id in test_stock_ids:\n    df = pd.DataFrame(book_preprocessor(delta, stock_id), columns = ['history']).reset_index()\n    test_dataset = test_dataset.append(df)\n    \ntest_dataset = test_dataset.reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T21:24:41.411381Z","iopub.execute_input":"2021-09-26T21:24:41.411706Z","iopub.status.idle":"2021-09-26T21:24:41.563362Z","shell.execute_reply.started":"2021-09-26T21:24:41.411679Z","shell.execute_reply":"2021-09-26T21:24:41.562566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 600\n\ntokenizer = RobertaTokenizerFast.from_pretrained('../input/krv-tokenizer')\n\nsibmiting_test = Triage(test_dataset, tokenizer, MAX_LEN, isSubmit = True)\nsubmit_params = {'batch_size': 1,\n                'num_workers': 0\n                }\nsubmit_loader = DataLoader(sibmiting_test, **submit_params)","metadata":{"execution":{"iopub.status.busy":"2021-09-26T21:24:45.530665Z","iopub.execute_input":"2021-09-26T21:24:45.530993Z","iopub.status.idle":"2021-09-26T21:24:45.57431Z","shell.execute_reply.started":"2021-09-26T21:24:45.530962Z","shell.execute_reply":"2021-09-26T21:24:45.573526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if cuda.is_available() else 'cpu'\nmodel_name = '../input/optiver-ml-final'\n\nwith open('../input/optiver-ml-final/model_new.pickle', 'rb') as f:\n    model = pickle.load(f)\nmodel.to(device)\n\ndef submit(model, submit_loader):\n    result = {}\n    model.eval()\n    with torch.no_grad():\n        for _, data in enumerate(submit_loader, 0):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            row_id = data['row_id'][0]\n\n            outputs = model(ids, mask)\n            big_val, big_idx = torch.max(outputs.data, dim=1)\n            result[row_id] = big_val.cpu().numpy()[0]\n\n    result = pd.DataFrame.from_dict(result, orient = 'index').reset_index()\n    result.columns = ['row_id', 'target']\n    return result\n\nresult = submit(model, submit_loader)\nresult.to_csv('submission.csv', index = False)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-26T21:24:54.181005Z","iopub.execute_input":"2021-09-26T21:24:54.181353Z","iopub.status.idle":"2021-09-26T21:25:04.719068Z","shell.execute_reply.started":"2021-09-26T21:24:54.18132Z","shell.execute_reply":"2021-09-26T21:25:04.718188Z"},"trusted":true},"execution_count":null,"outputs":[]}]}